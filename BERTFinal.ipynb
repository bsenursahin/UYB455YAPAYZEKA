{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsenursahin/UYB455YAPAYZEKA/blob/main/BERTFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFBKPrkPfic0"
      },
      "outputs": [],
      "source": [
        "#KÃ¼tÃ¼phaneleri kuruyoruz.\n",
        "\n",
        "!pip install transformers accelerate datasets scikit-learn openpyxl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNfndksmhBFu"
      },
      "outputs": [],
      "source": [
        "#BERT MODELÄ°\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# UyarÄ±larÄ± kapat\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training Configuration - A100 OPTIMIZED & EXCEL FIX\"\"\"\n",
        "\n",
        "    FILE_PATH = \"/content/temiz_veri_seti.xlsx\" # Excel dosyan\n",
        "\n",
        "    MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "    OUTPUT_DIR = \"./turkish_sentiment_model\"\n",
        "\n",
        "    TEXT_COLUMN = \"Yorum\"\n",
        "    LABEL_COLUMN = \"Duygu\"\n",
        "\n",
        "    # --- A100 PERFORMANS AYARLARI ---\n",
        "    MAX_LENGTH = 128\n",
        "    TRAIN_BATCH_SIZE = 256\n",
        "    EVAL_BATCH_SIZE = 512\n",
        "    NUM_EPOCHS = 3\n",
        "    LEARNING_RATE = 2e-5\n",
        "    WARMUP_RATIO = 0.1\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    TEST_SIZE = 0.10\n",
        "    VAL_SIZE = 0.10\n",
        "    RANDOM_STATE = 42\n",
        "    GRADIENT_ACCUMULATION_STEPS = 1\n",
        "    FP16 = True\n",
        "    GRADIENT_CHECKPOINTING = False\n",
        "    MAX_SAMPLES = None\n",
        "    NUM_WORKERS = 8\n",
        "\n",
        "\n",
        "def load_and_prepare_data(config):\n",
        "    \"\"\"Load and preprocess dataset (Excel & CSV Support)\"\"\"\n",
        "\n",
        "    print(\"Data Loading\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nLoading: {config.FILE_PATH}\")\n",
        "\n",
        "        # --- DÃœZELTME BURADA: Dosya uzantÄ±sÄ±na gÃ¶re okuma ---\n",
        "        if config.FILE_PATH.endswith('.xlsx') or config.FILE_PATH.endswith('.xls'):\n",
        "            print(\"Excel formatÄ± algÄ±landÄ±, pd.read_excel kullanÄ±lÄ±yor...\")\n",
        "            df = pd.read_excel(config.FILE_PATH)\n",
        "        else:\n",
        "            # CSV okuma denemeleri (eskisi gibi)\n",
        "            print(\"CSV formatÄ± varsayÄ±lÄ±yor, pd.read_csv kullanÄ±lÄ±yor...\")\n",
        "            try:\n",
        "                df = pd.read_csv(config.FILE_PATH, sep=';', on_bad_lines='skip', encoding='utf-8')\n",
        "            except:\n",
        "                try:\n",
        "                    df = pd.read_csv(config.FILE_PATH, sep=',', on_bad_lines='skip', encoding='utf-8')\n",
        "                except:\n",
        "                    df = pd.read_csv(config.FILE_PATH, sep=';', on_bad_lines='skip', encoding='windows-1254')\n",
        "\n",
        "        # SÃ¼tun isim temizliÄŸi\n",
        "        df.columns = df.columns.str.replace('Ã¯Â»Â¿', '', regex=False).str.strip()\n",
        "\n",
        "        # BoÅŸlarÄ± temizle\n",
        "        df = df.drop_duplicates(subset=[config.TEXT_COLUMN])\n",
        "        df = df.dropna(subset=[config.TEXT_COLUMN, config.LABEL_COLUMN])\n",
        "\n",
        "        print(f\"Total Rows: {len(df):,}\")\n",
        "\n",
        "        # Ã–rneklem (opsiyonel)\n",
        "        if config.MAX_SAMPLES and len(df) > config.MAX_SAMPLES:\n",
        "            df = df.sample(n=config.MAX_SAMPLES, random_state=config.RANDOM_STATE)\n",
        "\n",
        "        print(\"\\nMapping labels...\")\n",
        "        # EÄŸer etiketler zaten 0,1,2 ise dokunmaz, string ise Ã§evirir\n",
        "        label_map = {\n",
        "            'Olumsuz': 0, 'Negative': 0, 'negative': 0,\n",
        "            'Olumlu': 1, 'Positive': 1, 'positive': 1,\n",
        "            'NÃ¶tr': 2, 'Neutral': 2, 'neutral': 2, 'Notr': 2\n",
        "        }\n",
        "\n",
        "        # Etiket sÃ¼tunu string ise map'le, deÄŸilse olduÄŸu gibi bÄ±rak (integer kontrolÃ¼)\n",
        "        if df[config.LABEL_COLUMN].dtype == 'object':\n",
        "             df['label'] = df[config.LABEL_COLUMN].astype(str).str.strip().map(label_map)\n",
        "        else:\n",
        "             df['label'] = df[config.LABEL_COLUMN]\n",
        "\n",
        "        # Temizlik\n",
        "        df = df.dropna(subset=['label'])\n",
        "        df['label'] = df['label'].astype(int)\n",
        "\n",
        "        print(\"Class Distribution:\")\n",
        "        print(df['label'].value_counts().sort_index())\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"File not found: {config.FILE_PATH}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading data: {str(e)}\")\n",
        "\n",
        "\n",
        "def split_data(df, config):\n",
        "    print(\"\\nData Splitting\")\n",
        "    print(\"-\" * 70)\n",
        "    train_val, test = train_test_split(\n",
        "        df, test_size=config.TEST_SIZE,\n",
        "        random_state=config.RANDOM_STATE, stratify=df['label']\n",
        "    )\n",
        "    val_size_adjusted = config.VAL_SIZE / (1 - config.TEST_SIZE)\n",
        "    train, val = train_test_split(\n",
        "        train_val, test_size=val_size_adjusted,\n",
        "        random_state=config.RANDOM_STATE, stratify=train_val['label']\n",
        "    )\n",
        "    print(f\"Train: {len(train):,}\")\n",
        "    print(f\"Val:   {len(val):,}\")\n",
        "    print(f\"Test:  {len(test):,}\")\n",
        "    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def create_datasets(train_df, val_df, test_df, tokenizer, config):\n",
        "    print(\"\\nTokenization\")\n",
        "    print(\"-\" * 70)\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[config.TEXT_COLUMN],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=config.MAX_LENGTH\n",
        "        )\n",
        "    train_dataset = Dataset.from_pandas(train_df[[config.TEXT_COLUMN, 'label']])\n",
        "    val_dataset = Dataset.from_pandas(val_df[[config.TEXT_COLUMN, 'label']])\n",
        "    test_dataset = Dataset.from_pandas(test_df[[config.TEXT_COLUMN, 'label']])\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=5000)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, batch_size=5000)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=5000)\n",
        "\n",
        "    train_dataset = train_dataset.remove_columns([config.TEXT_COLUMN])\n",
        "    val_dataset = val_dataset.remove_columns([config.TEXT_COLUMN])\n",
        "    test_dataset = test_dataset.remove_columns([config.TEXT_COLUMN])\n",
        "\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1_macro\": f1_score(labels, predictions, average='macro'),\n",
        "        \"precision_macro\": precision_score(labels, predictions, average='macro'),\n",
        "        \"recall_macro\": recall_score(labels, predictions, average='macro')\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(config):\n",
        "    print(\"Turkish BERT Sentiment Analysis Training (A100 Optimized)\")\n",
        "    print(\"-\" * 70)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    df = load_and_prepare_data(config)\n",
        "    train_df, val_df, test_df = split_data(df, config)\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\nModel Loading\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        config.MODEL_NAME, num_labels=3,\n",
        "        id2label={0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"},\n",
        "        label2id={\"Negative\": 0, \"Positive\": 1, \"Neutral\": 2}\n",
        "    )\n",
        "\n",
        "    if config.GRADIENT_CHECKPOINTING:\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = create_datasets(\n",
        "        train_df, val_df, test_df, tokenizer, config\n",
        "    )\n",
        "    del train_df, val_df, test_df\n",
        "    gc.collect()\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.OUTPUT_DIR,\n",
        "        num_train_epochs=config.NUM_EPOCHS,\n",
        "        per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=config.LEARNING_RATE,\n",
        "        warmup_ratio=config.WARMUP_RATIO,\n",
        "        weight_decay=config.WEIGHT_DECAY,\n",
        "        logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        fp16=config.FP16,\n",
        "        dataloader_num_workers=config.NUM_WORKERS,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting Training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(trainer.evaluate())\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(trainer.evaluate(test_dataset))\n",
        "\n",
        "    print(\"\\nSaving Model...\")\n",
        "    trainer.save_model(config.OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
        "    print(\"Done.\")\n",
        "    return trainer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    trainer = train_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lbPtzH6iK7s"
      },
      "outputs": [],
      "source": [
        "# 1. Google Drive'Ä± BaÄŸla (Ä°zin isteyecek, onayla)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Modeli Drive'a 'tez_model_v1' adÄ±yla kopyala\n",
        "!cp -r ./turkish_sentiment_model /content/drive/MyDrive/tez_model_v1\n",
        "\n",
        "print(\"âœ… Model Google Drive'a baÅŸarÄ±yla yedeklendi: /content/drive/MyDrive/tez_model_v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "4K3Kg1UekSQx"
      },
      "outputs": [],
      "source": [
        "#Kaggle Ã¼zerindeki veri setini dÃ¼zenleyelim. (Dataset sÃ¼tunu kaldÄ±rÄ±lacak, label sayÄ±sallaÅŸtÄ±rÄ±lacak.)\n",
        "\n",
        "!pip install datasets pandas openpyxl -q\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "DATASET_NAME = \"winvoker/turkish-sentiment-analysis-dataset\"\n",
        "\n",
        "print(f\"{DATASET_NAME} indiriliyor...\")\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "dfs = []\n",
        "for split in dataset.keys():\n",
        "    dfs.append(pd.DataFrame(dataset[split]))\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Ä°lk SÃ¼tunlar: {df.columns.tolist()}\")\n",
        "\n",
        "if 'dataset' in df.columns:\n",
        "    df = df.drop(columns=['dataset'])\n",
        "    print(\"'dataset' sÃ¼tunu silindi.\")\n",
        "\n",
        "df = df.rename(columns={\n",
        "    \"text\": \"Yorum\",\n",
        "    \"label\": \"Duygu\"\n",
        "})\n",
        "\n",
        "label_map = {\n",
        "    'Negative': 0, 'negative': 0, 'Olumsuz': 0,\n",
        "    'Positive': 1, 'positive': 1, 'Olumlu': 1,\n",
        "    'Neutral': 2,  'neutral': 2,  'NÃ¶tr': 2, 'Notr': 2\n",
        "}\n",
        "\n",
        "if df['Duygu'].dtype == 'object':\n",
        "    df['Duygu'] = df['Duygu'].map(label_map)\n",
        "    print(\"Etiketler (positive/negative) sayÄ±sal formata (0/1/2) Ã§evrildi.\")\n",
        "\n",
        "df = df.dropna(subset=['Duygu', 'Yorum'])\n",
        "df['Duygu'] = df['Duygu'].astype(int)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Son Durum:\")\n",
        "print(df.head())\n",
        "print(\"\\nEtiket DaÄŸÄ±lÄ±mÄ±:\\n\", df['Duygu'].value_counts())\n",
        "\n",
        "dosya_adi = \"temiz_veri_seti.xlsx\"\n",
        "df.to_excel(dosya_adi, index=False)\n",
        "\n",
        "print(f\"\\nTAMAMLANDI! '{dosya_adi}' dosyasÄ±nÄ± sol menÃ¼den indirip eÄŸitimde kullanabilirsin.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "sutun_isimleri = [\"ID\", \"Category\", \"Review\", \"Sentiment\", \"Split\", \"Label\"]\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('/content/HUMIRSentimentDatasets.csv',\n",
        "                     header=None,          # Ä°lk satÄ±rÄ± baÅŸlÄ±k yapma, veri olarak al\n",
        "                     names=sutun_isimleri, # SÃ¼tun isimlerini biz veriyoruz\n",
        "                     sep=None,             # AyÄ±rÄ±cÄ±yÄ± (virgÃ¼l mÃ¼ tab mÄ±) otomatik algÄ±la\n",
        "                     engine='python',      # Python motoru hatalara karÅŸÄ± daha esnektir\n",
        "                     on_bad_lines='skip')  # HatalÄ± olan o 29. satÄ±rÄ± atla ve devam et\n",
        "\n",
        "    print(\"âœ… Veri baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nToplam Okunan SatÄ±r SayÄ±sÄ±: {len(df)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Hata: {e}\")"
      ],
      "metadata": {
        "id": "8aAkm_3yiPIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Veri Seti BÃ¶lÃ¼mleri (Split) ---\")\n",
        "print(df['Split'].value_counts())\n",
        "\n",
        "print(\"\\n--- Duygu DaÄŸÄ±lÄ±mÄ± (Sentiment) ---\")\n",
        "print(df['Sentiment'].value_counts())\n",
        "\n",
        "print(\"\\n--- Etiket DaÄŸÄ±lÄ±mÄ± (Label) ---\")\n",
        "print(df['Label'].value_counts())\n",
        "\n",
        "# Eksik veri kontrolÃ¼ (Ã¶zellikle Review sÃ¼tununda)\n",
        "print(f\"\\nBoÅŸ Yorum SayÄ±sÄ±: {df['Review'].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "srJ648IniZaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- 1. DOSYAYI OKUMA (En baÅŸtaki adÄ±m) ---\n",
        "print(\"1. Dosya okunuyor...\")\n",
        "sutun_isimleri = [\"ID\", \"Category\", \"Review\", \"Sentiment\", \"Split\", \"Label\"]\n",
        "try:\n",
        "    df = pd.read_csv('/content/HUMIRSentimentDatasets.csv',\n",
        "                     header=None,\n",
        "                     names=sutun_isimleri,\n",
        "                     sep=None,\n",
        "                     engine='python',\n",
        "                     on_bad_lines='skip')\n",
        "    print(\"Dosya baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "except Exception as e:\n",
        "    print(f\" Dosya okunurken hata: {e}\")\n",
        "\n",
        "# --- 2. VERÄ°YÄ° HAZIRLAMA ---\n",
        "print(\"2. Veriler dÃ¼zenleniyor...\")\n",
        "# Etiketleri SayÄ±ya Ã‡evirme\n",
        "label_map = {'Negative': 0, 'Positive': 1}\n",
        "df['target'] = df['Sentiment'].map(label_map)\n",
        "\n",
        "# Train ve Test olarak ayÄ±rma (Split sÃ¼tununu kullanarak)\n",
        "train_raw = df[df['Split'] == 'train'].copy()\n",
        "test_df = df[df['Split'] == 'test'].copy()\n",
        "\n",
        "# Validation oluÅŸturma (Train iÃ§inden %10 ayÄ±rÄ±yoruz)\n",
        "train_df, val_df = train_test_split(train_raw, test_size=0.1, random_state=42, stratify=train_raw['target'])\n",
        "\n",
        "# Tokenizer'Ä± yÃ¼kleme\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\" Tokenizer hazÄ±r.\")\n",
        "\n",
        "# --- 3. DATASET CLASS ve LOADER ---\n",
        "print(\"3. Veri YÃ¼kleyiciler oluÅŸturuluyor...\")\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.df.iloc[index]['Review'])\n",
        "        target = self.df.iloc[index]['target']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Ayarlar\n",
        "MAX_LEN = 160\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Datasetleri OluÅŸtur\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, MAX_LEN)\n",
        "val_dataset = ReviewDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer, MAX_LEN)\n",
        "\n",
        "# LoaderlarÄ± OluÅŸtur\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"âœ… HER ÅEY HAZIR! EÄŸitime geÃ§ebiliriz.\")"
      ],
      "metadata": {
        "id": "E2v9Y6-qlvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "BERT/RoBERTa Fine-Tuning Script - A100 Optimized\n",
        " 3 sÄ±nÄ±flÄ± modeli 2 sÄ±nÄ±fa uyarlar\n",
        " Yeni model olarak kaydeder (tez_model_v2)\n",
        " A100 GPU iÃ§in optimize edilmiÅŸ\n",
        " Hata kontrolÃ¼ %100\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW  #  DÃœZELTME: transformers'tan deÄŸil torch'tan import\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "import warnings\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    'ORIGINAL_MODEL_NAME': 'tez_model_v1',    # Drive'daki mevcut model\n",
        "    'NEW_MODEL_NAME': 'tez_model_v2',         # Yeni model adÄ±\n",
        "\n",
        "    'NUM_LABELS': 2,                          # Yeni veri setindeki sÄ±nÄ±f sayÄ±sÄ±\n",
        "    'LABEL_NAMES': ['Negative', 'Positive'],  # SÄ±nÄ±f isimleri\n",
        "\n",
        "    'EPOCHS': 3,\n",
        "    'BATCH_SIZE': None,  # None = otomatik tespit (val_loader'dan alÄ±r)\n",
        "    'LEARNING_RATE': 2e-5,\n",
        "    'WARMUP_RATIO': 0.1,\n",
        "    'MAX_GRAD_NORM': 1.0,\n",
        "    'WEIGHT_DECAY': 0.01,\n",
        "\n",
        "    'AUTO_SAVE': True,                        # True = otomatik kaydet\n",
        "    'SAVE_BEST_ONLY': False,                  # True = sadece en iyi modeli kaydet\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" BERT/RoBERTa FINE-TUNING BAÅLIYOR\")\n",
        "print(\"=\"*70)\n",
        "print(f\" Kaynak model    : {CONFIG['ORIGINAL_MODEL_NAME']}\")\n",
        "print(f\" Hedef model     : {CONFIG['NEW_MODEL_NAME']}\")\n",
        "print(f\" SÄ±nÄ±f sayÄ±sÄ±    : 3 â†’ {CONFIG['NUM_LABELS']}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 0. GÃœVENLÄ°K KONTROLLERI\n",
        "# =============================================================================\n",
        "\n",
        "# Gerekli deÄŸiÅŸkenleri kontrol et\n",
        "required_vars = {\n",
        "    'train_loader': 'EÄŸitim veri yÃ¼kleyici',\n",
        "    'val_loader': 'Validasyon veri yÃ¼kleyici',\n",
        "    'train_df': 'EÄŸitim dataframe'\n",
        "}\n",
        "\n",
        "missing = []\n",
        "for var_name, description in required_vars.items():\n",
        "    if var_name not in dir():\n",
        "        missing.append(f\" {var_name} ({description})\")\n",
        "    else:\n",
        "        print(f\"  âœ… {var_name} bulundu\")\n",
        "\n",
        "if missing:\n",
        "    print(\"\\n HATA: AÅŸaÄŸÄ±daki deÄŸiÅŸkenler bulunamadÄ±:\")\n",
        "    print(\"\\n\".join(missing))\n",
        "    print(\"\\nÃ‡Ã–ZÃœM: Ã–nce veri hazÄ±rlama kodunu Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "# Batch size'Ä± otomatik tespit et\n",
        "if CONFIG['BATCH_SIZE'] is None:\n",
        "    try:\n",
        "        CONFIG['BATCH_SIZE'] = train_loader.batch_size\n",
        "        print(f\"  Batch size otomatik tespit edildi: {CONFIG['BATCH_SIZE']}\")\n",
        "    except:\n",
        "        CONFIG['BATCH_SIZE'] = 16\n",
        "        print(f\"  Batch size tespit edilemedi, varsayÄ±lan: {CONFIG['BATCH_SIZE']}\")\n",
        "\n",
        "print(\"\\n TÃ¼m kontroller baÅŸarÄ±lÄ±!\")\n",
        "\n",
        "\n",
        "print(\"\\n Google Drive baÄŸlanÄ±yor...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"Drive baÄŸlandÄ±\")\n",
        "except Exception as e:\n",
        "    print(f\" Drive baÄŸlantÄ± uyarÄ±sÄ±: {e}\")\n",
        "    print(\"   (Zaten baÄŸlÄ±ysa sorun yok)\")\n",
        "\n",
        "print(\"\\n GPU kontrol ediliyor...\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\" HATA: GPU bulunamadÄ±!\")\n",
        "    print(\" Ã‡Ã–ZÃœM: Runtime > Change runtime type > T4/A100 GPU seÃ§in\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"GPU aktif: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# A100 iÃ§in Ã¶zel optimizasyonlar\n",
        "if \"A100\" in torch.cuda.get_device_name(0):\n",
        "    print(\"A100 tespit edildi - TF32 optimizasyonlarÄ± aktif!\")\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "original_path = f\"/content/drive/MyDrive/{CONFIG['ORIGINAL_MODEL_NAME']}\"\n",
        "print(f\"\\n Model yÃ¼kleniyor: {original_path}\")\n",
        "\n",
        "# Modelin var olduÄŸunu kontrol et\n",
        "if not os.path.exists(original_path):\n",
        "    print(f\" HATA: Model klasÃ¶rÃ¼ bulunamadÄ±!\")\n",
        "    print(f\"   Aranan konum: {original_path}\")\n",
        "    print(\"\\n Ã‡Ã–ZÃœM: Drive'Ä±nÄ±zda ÅŸu klasÃ¶r var mÄ± kontrol edin:\")\n",
        "    print(f\"   MyDrive/{CONFIG['ORIGINAL_MODEL_NAME']}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "try:\n",
        "    # Tokenizer'Ä± yÃ¼kle\n",
        "    print(\"  â†’ Tokenizer yÃ¼kleniyor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(original_path)\n",
        "\n",
        "    # Modeli yÃ¼kle ve 2 sÄ±nÄ±fa uyarla\n",
        "    print(\"  â†’ Model yÃ¼kleniyor ve 2 sÄ±nÄ±fa uyarlanÄ±yor...\")\n",
        "    print(\"     (3 sÄ±nÄ±flÄ± classifier katmanÄ± â†’ 2 sÄ±nÄ±flÄ± olarak yeniden oluÅŸturuluyor)\")\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        original_path,\n",
        "        num_labels=CONFIG['NUM_LABELS'],\n",
        "        ignore_mismatched_sizes=True  # Bu satÄ±r 3â†’2 uyarlamasÄ±nÄ± otomatik yapar\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"\\n Model baÅŸarÄ±yla yÃ¼klendi ve uyarlandÄ±!\")\n",
        "    print(f\"   Model tipi: {model.config.model_type.upper()}\")\n",
        "    print(f\"   SÄ±nÄ±f sayÄ±sÄ±: {model.num_labels}\")\n",
        "    print(f\"   Toplam parametre: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"   EÄŸitilebilir parametre: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Model yÃ¼kleme hatasÄ±!\")\n",
        "    print(f\"   Hata: {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "\n",
        "def get_predictions(model, data_loader, device):\n",
        "    \"\"\"Model tahminlerini ve gerÃ§ek deÄŸerleri dÃ¶ndÃ¼rÃ¼r\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Tahmin yapÄ±lÄ±yor\", leave=False):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    return true_labels, predictions\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Metrikleri hesapla\"\"\"\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, device):\n",
        "    \"\"\"Bir epoch eÄŸitim\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=\"EÄŸitim\", leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"targets\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=CONFIG['MAX_GRAD_NORM'])\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Metrikleri hesapla\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Progress bar gÃ¼ncelle\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'acc': f\"{correct/total:.4f}\"\n",
        "        })\n",
        "\n",
        "    return total_loss / len(data_loader), correct / total\n",
        "\n",
        "\n",
        "def eval_epoch(model, data_loader, loss_fn, device):\n",
        "    \"\"\"Validasyon epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validasyon\", leave=False):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader), correct / total\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" BASELINE PERFORMANS (EÄŸitim Ã–ncesi)\")\n",
        "print(\"=\"*70)\n",
        "print(\" Model performansÄ± Ã¶lÃ§Ã¼lÃ¼yor...\")\n",
        "\n",
        "y_true, y_pred_baseline = get_predictions(model, val_loader, device)\n",
        "baseline_metrics = calculate_metrics(y_true, y_pred_baseline)\n",
        "\n",
        "print(\"\\n SonuÃ§lar:\")\n",
        "for metric, value in baseline_metrics.items():\n",
        "    print(f\"  {metric.upper():12s}: {value:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“ EÄÄ°TÄ°M HAZIRLIÄI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['LEARNING_RATE'],\n",
        "    eps=1e-8,\n",
        "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
        ")\n",
        "\n",
        "# Scheduler\n",
        "total_steps = len(train_loader) * CONFIG['EPOCHS']\n",
        "warmup_steps = int(CONFIG['WARMUP_RATIO'] * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "print(f\" Optimizer hazÄ±r (AdamW, lr={CONFIG['LEARNING_RATE']})\")\n",
        "print(f\" Scheduler hazÄ±r (warmup={warmup_steps} steps)\")\n",
        "print(f\" Loss function hazÄ±r (CrossEntropyLoss)\")\n",
        "print(f\"\\n EÄŸitim detaylarÄ±:\")\n",
        "print(f\"  â€¢ Epoch sayÄ±sÄ±: {CONFIG['EPOCHS']}\")\n",
        "print(f\"  â€¢ Batch size: {CONFIG['BATCH_SIZE']}\")\n",
        "print(f\"  â€¢ Toplam step: {total_steps}\")\n",
        "print(f\"  â€¢ Warmup step: {warmup_steps}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" EÄÄ°TÄ°M BAÅLIYOR\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(CONFIG['EPOCHS']):\n",
        "    print(f\"\\nğŸ“… EPOCH {epoch + 1}/{CONFIG['EPOCHS']}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, loss_fn, optimizer, scheduler, device\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc = eval_epoch(model, val_loader, loss_fn, device)\n",
        "\n",
        "    # KayÄ±t\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Best model tracking\n",
        "    is_best = val_acc > best_val_acc\n",
        "    if is_best:\n",
        "        best_val_acc = val_acc\n",
        "\n",
        "    # SonuÃ§larÄ± yazdÄ±r\n",
        "    print(f\"\\n{'':20s} Loss      Accuracy\")\n",
        "    print(f\"  Train:         {train_loss:8.4f}  {train_acc:8.4f}\")\n",
        "    print(f\"  Validation:    {val_loss:8.4f}  {val_acc:8.4f} {'ğŸŒŸ BEST!' if is_best else ''}\")\n",
        "\n",
        "print(\"\\nâœ… EÄŸitim tamamlandÄ±!\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" FÄ°NAL PERFORMANS (EÄŸitim SonrasÄ±)\")\n",
        "print(\"=\"*70)\n",
        "print(\" Model performansÄ± Ã¶lÃ§Ã¼lÃ¼yor...\")\n",
        "\n",
        "y_true, y_pred_final = get_predictions(model, val_loader, device)\n",
        "final_metrics = calculate_metrics(y_true, y_pred_final)\n",
        "\n",
        "print(\"\\nğŸ“ˆ SonuÃ§lar:\")\n",
        "for metric, value in final_metrics.items():\n",
        "    print(f\"  {metric.upper():12s}: {value:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" BASELINE vs FÄ°NAL KARÅILAÅTIRMA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Metrik': ['Accuracy', 'F1 Score', 'Recall', 'Precision'],\n",
        "    'Baseline': [baseline_metrics[k] for k in ['accuracy', 'f1', 'recall', 'precision']],\n",
        "    'Final': [final_metrics[k] for k in ['accuracy', 'f1', 'recall', 'precision']]\n",
        "})\n",
        "\n",
        "comparison['Fark'] = comparison['Final'] - comparison['Baseline']\n",
        "comparison['DeÄŸiÅŸim (%)'] = ((comparison['Final'] / comparison['Baseline']) - 1) * 100\n",
        "\n",
        "print(comparison.to_string(index=False, formatters={\n",
        "    'Baseline': '{:.4f}'.format,\n",
        "    'Final': '{:.4f}'.format,\n",
        "    'Fark': '{:+.4f}'.format,\n",
        "    'DeÄŸiÅŸim (%)': '{:+.2f}%'.format\n",
        "}))\n",
        "\n",
        "# DetaylÄ± rapor\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETAYLI SINIFLANDIRMA RAPORU\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(y_true, y_pred_final,\n",
        "                          target_names=CONFIG['LABEL_NAMES'],\n",
        "                          digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\" Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_true, y_pred_final)\n",
        "print(cm)\n",
        "\n",
        "\n",
        "\n",
        "save_model = CONFIG['AUTO_SAVE']\n",
        "\n",
        "if not save_model:\n",
        "    response = input(\"\\nğŸ’¾ Modeli kaydetmek ister misiniz? (e/h): \").lower()\n",
        "    save_model = (response == 'e')\n",
        "\n",
        "if save_model:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" MODEL KAYDEDÄ°LÄ°YOR\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    save_path = f\"/content/drive/MyDrive/{CONFIG['NEW_MODEL_NAME']}\"\n",
        "\n",
        "    # KlasÃ¶r oluÅŸtur\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Model ve tokenizer'Ä± kaydet\n",
        "    print(f\" Kaydediliyor: {save_path}\")\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # Config dosyasÄ± kaydet\n",
        "    config_save = CONFIG.copy()\n",
        "    config_save['training_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    config_save['baseline_metrics'] = baseline_metrics\n",
        "    config_save['final_metrics'] = final_metrics\n",
        "    config_save['training_history'] = history\n",
        "    config_save['gpu_used'] = torch.cuda.get_device_name(0)\n",
        "\n",
        "    with open(f\"{save_path}/training_info.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(config_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\" Model kaydedildi!\")\n",
        "    print(f\" EÄŸitim bilgileri kaydedildi: training_info.json\")\n",
        "\n",
        "    print(\"\\n Kaydedilen dosyalar:\")\n",
        "    print(f\"  â€¢ Model: {save_path}/pytorch_model.bin\")\n",
        "    print(f\"  â€¢ Config: {save_path}/config.json\")\n",
        "    print(f\"  â€¢ Tokenizer: {save_path}/tokenizer_config.json\")\n",
        "    print(f\"  â€¢ Training Info: {save_path}/training_info.json\")\n",
        "else:\n",
        "    print(\"\\nâ­ Model kaydedilmedi\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Ä°ÅLEM TAMAMLANDI!\")\n",
        "print(\"=\"*70)\n",
        "print(f\" Kaynak model: {CONFIG['ORIGINAL_MODEL_NAME']}\")\n",
        "print(f\" Yeni model: {CONFIG['NEW_MODEL_NAME']}\")\n",
        "print(f\" SÄ±nÄ±f sayÄ±sÄ±: 3 â†’ {CONFIG['NUM_LABELS']}\")\n",
        "print(f\" Final Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "print(f\" GeliÅŸme: {comparison[comparison['Metrik']=='Accuracy']['DeÄŸiÅŸim (%)'].values[0]:+.2f}%\")\n",
        "\n",
        "if save_model:\n",
        "    print(f\"\\n Model ÅŸurada: MyDrive/{CONFIG['NEW_MODEL_NAME']}\")\n",
        "    print(\" Bir sonraki eÄŸitim iÃ§in: 'tez_model_v2' â†’ 'tez_model_v3'\")\n",
        "\n",
        "print(\"\\n Tebrikler! Her ÅŸey baÅŸarÄ±lÄ±!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "2IiDSf2wrUuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. ADIM: VERÄ°LERÄ° DENGELÄ° BÄ°R ÅEKÄ°LDE HAZIRLAMA (HATA DÃœZELTÄ°LMÄ°Å)\n",
        "print(\"Veriler okunuyor...\")\n",
        "\n",
        "# Ana veri setini oku\n",
        "df_main = pd.read_csv('/content/HUMIRSentimentDatasets.csv',\n",
        "                      header=None, names=[\"ID\", \"Category\", \"Review\", \"Sentiment\", \"Split\", \"Label\"],\n",
        "                      engine='python', on_bad_lines='skip')\n",
        "\n",
        "# --- HATA Ã–NLEME: Etiketleri temizle ve sayÄ±ya Ã§evir ---\n",
        "# BazÄ± CSV dosyalarÄ±nda label kÄ±smÄ±nda boÅŸluk kalabiliyor, onlarÄ± temizleyelim\n",
        "df_main['Label'] = df_main['Label'].astype(str).str.strip()\n",
        "df_main['Label'] = pd.to_numeric(df_main['Label'], errors='coerce') # SayÄ±ya Ã§evir, hata varsa NaN yap\n",
        "df_main = df_main.dropna(subset=['Label']) # HatalÄ± satÄ±rlarÄ± sil\n",
        "df_main['Label'] = df_main['Label'].astype(int)\n",
        "\n",
        "# Pozitifi 2 yap (Senin 3 sÄ±nÄ±flÄ± sisteme uyum)\n",
        "df_main['Label'] = df_main['Label'].replace({1: 2})\n",
        "\n",
        "print(\"Ana veri seti daÄŸÄ±lÄ±mÄ±:\", df_main['Label'].value_counts().to_dict())\n",
        "\n",
        "# --- KÃœÃ‡ÃœLTME (DOWNSAMPLING) GÃœVENLÄ° HALE GETÄ°RÄ°LDÄ° ---\n",
        "# EÄŸer sÄ±nÄ±fta 4000'den az veri varsa hata vermemesi iÃ§in min() kullanÄ±yoruz\n",
        "n_sample = 4000\n",
        "neg_count = min(len(df_main[df_main['Label'] == 0]), n_sample)\n",
        "pos_count = min(len(df_main[df_main['Label'] == 2]), n_sample)\n",
        "\n",
        "df_neg_small = df_main[df_main['Label'] == 0].sample(n=neg_count, random_state=42)\n",
        "df_pos_small = df_main[df_main['Label'] == 2].sample(n=pos_count, random_state=42)\n",
        "\n",
        "# Senin 1000 verini oku\n",
        "df_gold = pd.read_excel('/content/ironi_veri_seti.xlsx')\n",
        "df_gold = df_gold[['Review', 'target']]\n",
        "df_gold.columns = ['Review', 'Label']\n",
        "\n",
        "# Senin verilerini 5 katÄ±na Ã§Ä±kararak nÃ¶trleri gÃ¼Ã§lendir\n",
        "df_gold_upsampled = pd.concat([df_gold] * 5, ignore_index=True)\n",
        "\n",
        "# BirleÅŸtirme\n",
        "df_final = pd.concat([df_neg_small, df_pos_small, df_gold_upsampled], ignore_index=True)\n",
        "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"ğŸ“Š Final Dengeli DaÄŸÄ±lÄ±m:\\n{df_final['Label'].value_counts()}\")"
      ],
      "metadata": {
        "id": "cGABveVBVnFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODELÄ° GERÃ‡EK SINAVINA SOKUYORUZ ---\n",
        "zor_test_listesi = [\n",
        "    \"ÃœrÃ¼nÃ¼n kutusu gri renkte ve Ã¼zerinde marka logosu var.\", # Saf NÃ¶tr (Buna pozitif derse hala sÄ±kÄ±ntÄ± var demektir)\n",
        "    \"Kargom bugÃ¼n teslim edildi.\", # Saf NÃ¶tr\n",
        "    \"MaÅŸallah kurye o kadar yavaÅŸ ki kaplumbaÄŸalar yanÄ±ndan vÄ±z vÄ±z geÃ§iyor.\", # Sert Ä°roni\n",
        "    \"Ä°nanÄ±lmaz bir hizmet, paramÄ±zla rezil olduk.\", # Ã‡ok Net Ä°roni\n",
        "    \"TeÅŸekkÃ¼rler, sayenizde bÃ¼tÃ¼n gÃ¼n bekledim.\", # Ä°nce Ä°roni\n",
        "    \"CihazÄ±n bataryasÄ± 5000 mAh kapasiteye sahip.\" # Saf Teknik Bilgi (NÃ¶tr)\n",
        "]\n",
        "\n",
        "print(\" DERÄ°N ANALÄ°Z BAÅLIYOR...\\n\")\n",
        "for t in zor_test_listesi:\n",
        "    print(f\"Metin: {t}\\nTahmin: {tahmin_et(t)}\\n{'-'*30}\")"
      ],
      "metadata": {
        "id": "a9glCiy5bD78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODELÄ° TERLETEN EKSTRA ZOR TEST ---\n",
        "akademik_test_listesi = [\n",
        "    \"SayÄ±n yetkili, talebiniz ilgili birime iletilmiÅŸ olup inceleme sÃ¼reci devam etmektedir.\", # Resmi NÃ¶tr\n",
        "    \"Harika bir plan, hafta sonunu evde temizlik yaparak geÃ§ireceÄŸim iÃ§in Ã§ok mutluyum!\", # Sosyal Ä°roni\n",
        "    \"KitabÄ±n sayfa sayÄ±sÄ± 320, kapak tasarÄ±mÄ± ise mat selefon kaplama.\", # Saf Betimleyici NÃ¶tr\n",
        "    \"Aman ne gÃ¼zel, bir bu eksikti zaten, tam oldu.\", # KÄ±sa ve Keskin Ä°roni\n",
        "    \"Hizmet kalitesi beklentilerimin Ã§ok altÄ±nda kaldÄ±, hayal kÄ±rÄ±klÄ±ÄŸÄ±.\", # DÃ¼z Negatif (Ä°roni deÄŸil, direkt eleÅŸtiri)\n",
        "    \"Dokunmatik ekran hassasiyeti ve panel parlaklÄ±ÄŸÄ± standart seviyede.\", # Teknik NÃ¶tr\n",
        "    \"Muazzam bir zeka Ã¶rneÄŸi, anahtarÄ± evin iÃ§inde unutmayÄ± nasÄ±l baÅŸardÄ±n?\", # KiÅŸisel Ä°roni\n",
        "    \"Hava sÄ±caklÄ±ÄŸÄ± mevsim normallerinin 5 derece Ã¼zerinde seyrediyor.\" # DoÄŸa/Haber NÃ¶tr\n",
        "]\n",
        "\n",
        "print(\" AKADEMÄ°K SORGULAMA BAÅLIYOR...\\n\")\n",
        "for t in akademik_test_listesi:\n",
        "    print(f\"Metin: {t}\\nTahmin: {tahmin_et(t)}\\n{'-'*30}\")"
      ],
      "metadata": {
        "id": "yJLZ39ipbXC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rnPLODdDXA_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODELÄ° SINIRLARINA ZORLAYAN ABSÃœRT TEST ---\n",
        "absurt_test_listesi = [\n",
        "    \"MaÅŸallah, atomu parÃ§alayÄ±p iÃ§ine peynir ekmek koymuÅŸsun, dÃ¢hice!\", # AÄŸÄ±r alay/Ä°roni\n",
        "    \"DikdÃ¶rtgen prizmanÄ±n iÃ§ aÃ§Ä±larÄ± toplamÄ± ile ilgili bir makale okuyorum.\", # AÅŸÄ±rÄ± NÃ¶tr/Akademik\n",
        "    \"Tebrikler, beÅŸ dakikalÄ±k yolu Ã¼Ã§ saatte gelerek dÃ¼nya rekoru kÄ±rdÄ±n.\", # Zaman ironisi\n",
        "    \"Mutfaktaki musluktan su akÄ±yor ve lavabonun rengi beyaz.\", # Ã‡ok saÃ§ma derecede nÃ¶tr\n",
        "    \"Harika bir iÅŸ Ã§Ä±kardÄ±n, sayende bÃ¼tÃ¼n proje Ã§Ã¶p oldu ve herkes kovuldu.\", # YÄ±kÄ±cÄ± Ä°roni\n",
        "    \"Elektrikli sÃ¼pÃ¼rgenin emiÅŸ gÃ¼cÃ¼ 2000 watt olup, kablo uzunluÄŸu 5 metredir.\", # Teknik NÃ¶tr\n",
        "    \"Vay be, kaplumbaÄŸaya binseydik ÅŸu an Ã§oktan varmÄ±ÅŸtÄ±k, hÄ±zÄ±na hayran kaldÄ±m.\", # HÄ±z ironisi\n",
        "    \"Masadaki bardaÄŸÄ±n iÃ§inde yarÄ±m bardak ÅŸeffaf su bulunuyor.\" # GÃ¶zlem NÃ¶tr\n",
        "]\n",
        "\n",
        "print(\" ABSÃœRT VE KARMAÅIK TEST BAÅLIYOR...\\n\")\n",
        "for t in absurt_test_listesi:\n",
        "    print(f\"Metin: {t}\\nTahmin: {tahmin_et(t)}\\n{'-'*30}\")"
      ],
      "metadata": {
        "id": "1zfVYJ4ubti2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. GOOGLE DRIVE BAÄLANTISI\n",
        "# ==========================================\n",
        "print(\"[INFO] Google Drive baÄŸlanÄ±yor...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ==========================================\n",
        "# 2. AYARLAR (CONFIGURATION)\n",
        "# ==========================================\n",
        "\n",
        "# Veri seti: Colab'e yÃ¼klediÄŸin dosya\n",
        "DATASET_PATH = \"/content/golden_dataset.xlsx\"\n",
        "\n",
        "# Modeller: Drive'daki yollar\n",
        "PREV_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v3_final\"\n",
        "OUTPUT_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v4_final\"\n",
        "\n",
        "# Egitim Parametreleri\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32          # A100 icin optimize\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# ==========================================\n",
        "# 3. DONANIM KONTROLÃœ\n",
        "# ==========================================\n",
        "print(\"\\n[INFO] Checking hardware resources...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"[INFO] GPU Active: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"[WARNING] GPU not found! Training will be slow.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Ã–NCEKÄ° MODELÄ° (v3) YÃœKLE\n",
        "# ==========================================\n",
        "print(f\"\\n[INFO] Loading previous model from: {PREV_MODEL_PATH}\")\n",
        "\n",
        "if not os.path.exists(PREV_MODEL_PATH):\n",
        "    print(f\"[CRITICAL ERROR] Model klasÃ¶rÃ¼ bulunamadÄ±: {PREV_MODEL_PATH}\")\n",
        "    print(\"   LÃ¼tfen Drive'da 'tez_model_v3_final' klasÃ¶rÃ¼nÃ¼n olduÄŸundan emin ol.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(PREV_MODEL_PATH)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(PREV_MODEL_PATH)\n",
        "    print(\"[INFO] Model v3 loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Model yÃ¼klenirken hata oluÅŸtu: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==========================================\n",
        "# 5. VERÄ° SETÄ°NÄ° YÃœKLE VE Ä°ÅLE\n",
        "# ==========================================\n",
        "print(f\"\\n[INFO] Loading dataset from: {DATASET_PATH}\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    print(f\"[ERROR] Veri seti bulunamadÄ±: {DATASET_PATH}\")\n",
        "    print(\"   LÃ¼tfen dosyayÄ± Colab'in sol tarafÄ±ndaki dosya yÃ¶neticisine yÃ¼klediÄŸinden emin ol.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Okuma\n",
        "if DATASET_PATH.endswith('.xlsx'):\n",
        "    df = pd.read_excel(DATASET_PATH)\n",
        "else:\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "# SÃ¼tun Ä°simlerini DÃ¼zeltme\n",
        "column_mapping = {\n",
        "    'Review': 'text',\n",
        "    'Sentiment': 'original_label',\n",
        "    'Label': 'original_label'\n",
        "}\n",
        "df = df.rename(columns=column_mapping)\n",
        "\n",
        "if 'text' not in df.columns or 'original_label' not in df.columns:\n",
        "    print(f\"[ERROR] Gerekli sÃ¼tunlar yok. Mevcutlar: {list(df.columns)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Etiket DÃ¶nÃ¼ÅŸÃ¼mÃ¼ (0, 1, 2)\n",
        "print(\"[INFO] Mapping labels...\")\n",
        "\n",
        "def map_sentiment(val):\n",
        "    val = str(val).lower().strip()\n",
        "    if val in ['negative', 'negatif', 'bad', 'kÃ¶tÃ¼', '0', '-1']: return 0\n",
        "    elif val in ['neutral', 'notr', 'nÃ¶tr', 'normal', '1']: return 1\n",
        "    elif val in ['positive', 'pozitif', 'good', 'iyi', '2']: return 2\n",
        "    return None\n",
        "\n",
        "df['label'] = df['original_label'].apply(map_sentiment)\n",
        "df = df.dropna(subset=['label', 'text'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "print(f\"[INFO] Data ready. Rows: {len(df)}\")\n",
        "print(f\"[INFO] Class Distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. EÄÄ°TÄ°M HAZIRLIÄI\n",
        "# ==========================================\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self): return len(self.labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
        "\n",
        "# ==========================================\n",
        "# 7. EÄÄ°TÄ°MÄ° BAÅLAT\n",
        "# ==========================================\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_v4',\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    eval_strategy=\"epoch\",  # DÃœZELTÄ°LDÄ°: evaluation_strategy -> eval_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"\\n[INFO] Starting training session...\")\n",
        "trainer.train()\n",
        "\n",
        "# ==========================================\n",
        "# 8. KAYDET\n",
        "# ==========================================\n",
        "print(f\"\\n[INFO] Saving model to: {OUTPUT_MODEL_PATH}\")\n",
        "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)\n",
        "\n",
        "print(\"[INFO] Process completed successfully.\")"
      ],
      "metadata": {
        "id": "d-QaSLw3A8DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# 1. KaydettiÄŸimiz Modeli YÃ¼kle\n",
        "MODEL_PATH = \"/content/drive/MyDrive/tez_model_v4_final\"\n",
        "print(f\"â³ Model yÃ¼kleniyor: {MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "    print(\" Model yÃ¼klendi ve teste hazÄ±r!\")\n",
        "except Exception as e:\n",
        "    print(f\" Hata: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 2. Zorlu Test CÃ¼mleleri (BunlarÄ± modelin kafasÄ±nÄ± karÄ±ÅŸtÄ±rmak iÃ§in seÃ§tim)\n",
        "test_sentences = [\n",
        "    \"ÃœrÃ¼n elime ulaÅŸtÄ± ama beklediÄŸimden biraz farklÄ±, yine de iÅŸ gÃ¶rÃ¼r.\",  # NÃ¶tr veya Pozitif arasÄ± (Zor)\n",
        "    \"HayatÄ±mda gÃ¶rdÃ¼ÄŸÃ¼m en rezalet paketleme, tebrik ederim gerÃ§ekten!\",    # Negatif (Ä°roni iÃ§eriyor)\n",
        "    \"Ne iyi ne kÃ¶tÃ¼, standart bir Ã¼rÃ¼n.\",                                  # NÃ¶tr\n",
        "    \"MÃ¼kemmel bir deneyimdi, herkese tavsiye ederim.\",                     # Pozitif\n",
        "    \"Kargo hÄ±zlÄ±ydÄ± ama Ã¼rÃ¼n kÄ±rÄ±k geldi.\",                                # Negatif\n",
        "    \"SatÄ±cÄ± Ã§ok ilgiliydi, teÅŸekkÃ¼rler.\",                                  # Pozitif\n",
        "]\n",
        "\n",
        "# 3. Tahminleri Yap\n",
        "print(\"\\n CANLI TEST SONUÃ‡LARI:\")\n",
        "print(\"-\" * 50)\n",
        "label_map = {\"LABEL_0\": \"Negatif\", \"LABEL_1\": \"NÃ¶tr\", \"LABEL_2\": \"Pozitif\"}\n",
        "\n",
        "for text in test_sentences:\n",
        "    result = classifier(text)[0]\n",
        "    label_code = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    human_readable = label_map.get(label_code, label_code)\n",
        "\n",
        "    print(f\" CÃ¼mle: {text}\")\n",
        "    print(f\" Tahmin: {human_readable} (GÃ¼ven: %{score*100:.2f})\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "XDmSm5EdBVji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. AYARLAR\n",
        "# ==========================================\n",
        "print(\"[INFO] Google Drive baÄŸlanÄ±yor...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# DOSYA YOLLARI\n",
        "GOLDEN_DATA_PATH = \"/content/golden_dataset.xlsx\"\n",
        "OLD_DATA_PATH    = \"/content/HUMIRSentimentDatasets.csv\"\n",
        "\n",
        "# MODEL YOLLARI\n",
        "PREV_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v3_final\"\n",
        "OUTPUT_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v5_balanced\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. VERÄ°LERÄ° YÃœKLE VE BÄ°RLEÅTÄ°R (DÃœZELTÄ°LDÄ°)\n",
        "# ==========================================\n",
        "print(\"\\n[INFO] Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "# --- A) Golden Dataset YÃ¼kle ---\n",
        "if os.path.exists(GOLDEN_DATA_PATH):\n",
        "    if GOLDEN_DATA_PATH.endswith('.xlsx'):\n",
        "        df_gold = pd.read_excel(GOLDEN_DATA_PATH)\n",
        "    else:\n",
        "        df_gold = pd.read_csv(GOLDEN_DATA_PATH)\n",
        "\n",
        "    # SÃ¼tunlarÄ± dÃ¼zelt\n",
        "    df_gold = df_gold.rename(columns={'Review': 'text', 'Sentiment': 'original_label', 'Label': 'original_label'})\n",
        "    df_gold = df_gold[['text', 'original_label']]\n",
        "    print(f\"   -> Golden Dataset: {len(df_gold)} satÄ±r eklendi.\")\n",
        "else:\n",
        "    print(f\"[ERROR] Golden Dataset bulunamadÄ±: {GOLDEN_DATA_PATH}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- B) Eski (Ä°roni) Dataset YÃ¼kle (SAÄLAMLAÅTIRILDI) ---\n",
        "df_old = pd.DataFrame() # VarsayÄ±lan boÅŸ\n",
        "\n",
        "if os.path.exists(OLD_DATA_PATH):\n",
        "    print(f\"   -> HUMIR Dataset okunuyor ({OLD_DATA_PATH})...\")\n",
        "    try:\n",
        "        # Ã–nce en yaygÄ±n formatÄ± dene: NoktalÄ± virgÃ¼l ve UTF-8\n",
        "        df_old = pd.read_csv(OLD_DATA_PATH, sep=';', encoding='utf-8', on_bad_lines='skip')\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            # Olmazsa TÃ¼rkÃ§e Windows formatÄ±nÄ± dene\n",
        "            df_old = pd.read_csv(OLD_DATA_PATH, sep=';', encoding='windows-1254', on_bad_lines='skip')\n",
        "        except Exception as e:\n",
        "            print(f\"[WARNING] Okuma hatasÄ±: {e}\")\n",
        "\n",
        "    # EÄŸer yukarÄ±dakiler boÅŸ geldiyse veya sÃ¼tunlarÄ± bulamadÄ±ysa virgÃ¼l dene (ama bad_lines skip ile)\n",
        "    if 'Review' not in df_old.columns and 'text' not in df_old.columns:\n",
        "        print(\"      (NoktalÄ± virgÃ¼l iÅŸe yaramadÄ±, virgÃ¼l deneniyor...)\")\n",
        "        try:\n",
        "            df_old = pd.read_csv(OLD_DATA_PATH, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # SÃ¼tunlarÄ± StandartlaÅŸtÄ±r\n",
        "    if 'Review' in df_old.columns: df_old = df_old.rename(columns={'Review': 'text'})\n",
        "    if 'Label' in df_old.columns: df_old = df_old.rename(columns={'Label': 'original_label'})\n",
        "    if 'target' in df_old.columns: df_old = df_old.rename(columns={'target': 'original_label'})\n",
        "\n",
        "    # Gereksiz sÃ¼tunlarÄ± at ve kontrol et\n",
        "    if 'text' in df_old.columns and 'original_label' in df_old.columns:\n",
        "        df_old = df_old[['text', 'original_label']]\n",
        "        print(f\"   -> HUMIR (Eski) Dataset BaÅŸarÄ±yla Eklendi: {len(df_old)} satÄ±r.\")\n",
        "    else:\n",
        "        print(f\"[ERROR] Eski veri setinde 'text' veya 'label' sÃ¼tunu bulunamadÄ±! SÃ¼tunlar: {list(df_old.columns)}\")\n",
        "        print(\"      LÃ¼tfen CSV dosyanÄ±zÄ±n iÃ§ini kontrol edin. SÃ¼tun adlarÄ± 'Review' ve 'Label' olmalÄ±.\")\n",
        "        # Devam etmesin ki boÅŸ veriyle eÄŸitilmesin\n",
        "        df_old = pd.DataFrame()\n",
        "else:\n",
        "    print(f\"[WARNING] Eski veri seti bulunamadÄ±: {OLD_DATA_PATH}\")\n",
        "\n",
        "# --- C"
      ],
      "metadata": {
        "id": "7nT3jNmTEcHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. AYARLAR\n",
        "# ==========================================\n",
        "print(\"[INFO] Google Drive baÄŸlanÄ±yor...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# DOSYA YOLLARI\n",
        "GOLDEN_DATA_PATH = \"/content/golden_dataset.xlsx\"\n",
        "OLD_DATA_PATH    = \"/content/HUMIRSentimentDatasets.csv\"\n",
        "\n",
        "# MODEL YOLLARI\n",
        "PREV_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v3_final\"\n",
        "OUTPUT_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v5_balanced\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. VERÄ°LERÄ° YÃœKLE VE AKILLI BÄ°RLEÅTÄ°R\n",
        "# ==========================================\n",
        "print(\"\\n[INFO] Veri setleri hazÄ±rlanÄ±yor...\")\n",
        "\n",
        "# --- A) Golden Dataset (NÃ¶tr Kalemiz) ---\n",
        "if os.path.exists(GOLDEN_DATA_PATH):\n",
        "    if GOLDEN_DATA_PATH.endswith('.xlsx'):\n",
        "        df_gold = pd.read_excel(GOLDEN_DATA_PATH)\n",
        "    else:\n",
        "        df_gold = pd.read_csv(GOLDEN_DATA_PATH)\n",
        "\n",
        "    df_gold = df_gold.rename(columns={'Review': 'text', 'Sentiment': 'original_label', 'Label': 'original_label'})\n",
        "    df_gold = df_gold[['text', 'original_label']]\n",
        "    print(f\"   -> Golden Dataset: {len(df_gold)} satÄ±r (NÃ¶trler burada gÃ¼vende).\")\n",
        "else:\n",
        "    print(f\"[ERROR] Golden Dataset bulunamadÄ±.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- B) HUMIR Dataset (Baharat) ---\n",
        "if os.path.exists(OLD_DATA_PATH):\n",
        "    print(f\"   -> HUMIR Dataset okunuyor...\")\n",
        "    try:\n",
        "        # BaÅŸlÄ±ksÄ±z okuma\n",
        "        df_old = pd.read_csv(OLD_DATA_PATH, sep=';', header=None, usecols=[2, 3], on_bad_lines='skip')\n",
        "        df_old.columns = ['text', 'original_label']\n",
        "\n",
        "        # --- KRÄ°TÄ°K DENGE AYARI ---\n",
        "        # Senin uyarÄ±n Ã¼zerine burayÄ± dÃ¼zelttik.\n",
        "        # Golden'Ä±n toplam sayÄ±sÄ±nÄ±n sadece %40'Ä± kadar eski veri alÄ±yoruz.\n",
        "        # BÃ¶ylece NÃ¶tr sÄ±nÄ±fÄ± ezilmeyecek, ama model eski ironileri de hatÄ±rlayacak.\n",
        "\n",
        "        ratio = 0.40  # %40 Oran (Ä°deal Denge)\n",
        "        target_count = int(len(df_gold) * ratio)\n",
        "\n",
        "        if len(df_old) > target_count:\n",
        "            print(f\"   âš–ï¸ DENGELEME: NÃ¶tr sÄ±nÄ±fÄ±nÄ± korumak iÃ§in eski veriden sadece {target_count} satÄ±r (Golden'Ä±n %40'Ä±) alÄ±nÄ±yor.\")\n",
        "            df_old = df_old.sample(n=target_count, random_state=42)\n",
        "\n",
        "        print(f\"   -> HUMIR Dataset Eklendi: {len(df_old)} satÄ±r.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HUMIR okunamadÄ±: {e}\")\n",
        "        df_old = pd.DataFrame()\n",
        "else:\n",
        "    df_old = pd.DataFrame()\n",
        "\n",
        "# --- C) BÄ°RLEÅTÄ°RME ---\n",
        "if not df_old.empty:\n",
        "    df = pd.concat([df_gold, df_old], ignore_index=True)\n",
        "else:\n",
        "    df = df_gold\n",
        "\n",
        "# KarÄ±ÅŸtÄ±r\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ETÄ°KETLERÄ° DÃ–NÃœÅTÃœR (0, 1, 2)\n",
        "# ==========================================\n",
        "print(\"[INFO] Etiketler iÅŸleniyor...\")\n",
        "\n",
        "def map_sentiment(val):\n",
        "    val = str(val).lower().strip()\n",
        "    if val in ['negative', 'negatif', 'bad', 'kÃ¶tÃ¼', '0', '-1', 'ironi', 'sarcasm']: return 0\n",
        "    elif val in ['neutral', 'notr', 'nÃ¶tr', 'normal', '1']: return 1\n",
        "    elif val in ['positive', 'pozitif', 'good', 'iyi', '2']: return 2\n",
        "    return None\n",
        "\n",
        "df['label'] = df['original_label'].apply(map_sentiment)\n",
        "df = df.dropna(subset=['label', 'text'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±nÄ± Kontrol Et (Burada NÃ¶tr sayÄ±sÄ± diÄŸerlerine yakÄ±n olmalÄ±)\n",
        "print(f\"ğŸ“Š [Ã–NEMLÄ°] Final SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:\\n{df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. EÄÄ°TÄ°M (v3 ÃœZERÄ°NE)\n",
        "# ==========================================\n",
        "print(f\"\\n[INFO] Model v3 yÃ¼kleniyor: {PREV_MODEL_PATH}\")\n",
        "\n",
        "if not os.path.exists(PREV_MODEL_PATH):\n",
        "    print(\"[ERROR] v3 Modeli bulunamadÄ±.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(PREV_MODEL_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(PREV_MODEL_PATH, num_labels=3, ignore_mismatched_sizes=True)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'].tolist(), df['label'].tolist(), test_size=0.10, random_state=42\n",
        ")\n",
        "\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self): return len(self.labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_v5',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n",
        ")\n",
        "\n",
        "print(\"\\n [v5] DENGELÄ° EÄÄ°TÄ°M BAÅLIYOR...\")\n",
        "trainer.train()\n",
        "\n",
        "print(f\"\\n Final Dengeli Model (v5) Kaydedildi: {OUTPUT_MODEL_PATH}\")\n",
        "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)"
      ],
      "metadata": {
        "id": "zScW9CoOHGkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# 1. Model Yolunu Belirle (v6 Final Base Model)\n",
        "MODEL_PATH = \"/content/drive/MyDrive/tez_model_v6_final_base\"\n",
        "print(f\" Model yÃ¼kleniyor: {MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
        "    print(\" Model yÃ¼klendi ve Ä°RONÄ° testine hazÄ±r!\")\n",
        "except Exception as e:\n",
        "    print(f\" Hata: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 2. SADECE Ä°RONÄ° CÃœMLELERÄ° (Zorluk Seviyesi: YÃ¼ksek)\n",
        "irony_sentences = [\n",
        "    \"HayatÄ±mda gÃ¶rdÃ¼ÄŸÃ¼m en rezalet paketleme, tebrik ederim gerÃ§ekten!\",\n",
        "    \"SipariÅŸ 10 gÃ¼nde geldi, Ä±ÅŸÄ±k hÄ±zÄ±ndasÄ±nÄ±z maÅŸallah.\",\n",
        "    \"Bozuk Ã¼rÃ¼n gÃ¶nderme konusunda bir dÃ¼nya markasÄ±sÄ±nÄ±z.\",\n",
        "    \"MÃ¼ÅŸteri hizmetleri o kadar ilgili ki telefonlara bakmaya tenezzÃ¼l etmiyorlar.\",\n",
        "    \"Harika, bir bu eksikti, tam da bozuk gÃ¶nderilmesini istemiÅŸtim!\",\n",
        "    \"ÃœrÃ¼n paramparÃ§a geldi, kargo firmasÄ±na bu Ã¶zenli taÅŸÄ±madan dolayÄ± madalya takmak lazÄ±m.\",\n",
        "    \"Yemek buz gibiydi, dondurma niyetine yedik saÄŸ olun.\",\n",
        "    \"Bu kadar kalitesiz bir Ã¼rÃ¼nÃ¼ Ã¼retmek de ayrÄ± bir baÅŸarÄ±, kutluyorum.\"\n",
        "]\n",
        "\n",
        "# 3. Testi BaÅŸlat\n",
        "print(\"\\nğŸ” Ä°RONÄ° TESTÄ° SONUÃ‡LARI:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Etiket HaritasÄ± (Negatif -> 0, NÃ¶tr -> 1, Pozitif -> 2)\n",
        "label_map = {\n",
        "    \"LABEL_0\": \" NEGATÄ°F (DoÄŸru Cevap)\",\n",
        "    \"LABEL_1\": \" NÃ–TR (HatalÄ±)\",\n",
        "    \"LABEL_2\": \" POZÄ°TÄ°F (HatalÄ± - Ä°roniyi AnlamadÄ±)\"\n",
        "}\n",
        "\n",
        "for text in irony_sentences:\n",
        "    result = classifier(text)[0]\n",
        "    label_code = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    human_readable = label_map.get(label_code, label_code)\n",
        "\n",
        "    print(f\"ğŸ“ CÃ¼mle: {text}\")\n",
        "    print(f\"ğŸ¤– Tahmin: {human_readable} (GÃ¼ven: %{score*100:.2f})\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "GuoHWDZHH8Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. AYARLAR\n",
        "# ==========================================\n",
        "print(\"[INFO] Google Drive baÄŸlanÄ±yor...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "GOLDEN_DATA_PATH = \"/content/golden_dataset.xlsx\"\n",
        "OLD_DATA_PATH    = \"/content/HUMIRSentimentDatasets.csv\"\n",
        "BASE_MODEL_NAME  = \"dbmdz/bert-base-turkish-cased\"\n",
        "OUTPUT_MODEL_PATH = \"/content/drive/MyDrive/tez_model_v7_neutral_boosted\" # v7 Ä°smi\n",
        "\n",
        "# ==========================================\n",
        "# 2. VERÄ°LERÄ° HAZIRLA VE NÃ–TR'Ãœ Ã‡OÄALT\n",
        "# ==========================================\n",
        "print(\"\\n[INFO] Veri Kokteyli HazÄ±rlanÄ±yor...\")\n",
        "\n",
        "# --- A) Golden Dataset ---\n",
        "if os.path.exists(GOLDEN_DATA_PATH):\n",
        "    df_gold = pd.read_excel(GOLDEN_DATA_PATH) if GOLDEN_DATA_PATH.endswith('.xlsx') else pd.read_csv(GOLDEN_DATA_PATH)\n",
        "    df_gold = df_gold.rename(columns={'Review': 'text', 'Sentiment': 'original_label', 'Label': 'original_label'})\n",
        "    df_gold = df_gold[['text', 'original_label']]\n",
        "else:\n",
        "    sys.exit(\"[ERROR] Golden Dataset yok!\")\n",
        "\n",
        "# --- B) HUMIR Dataset (Eski Veri) ---\n",
        "df_old = pd.DataFrame()\n",
        "if os.path.exists(OLD_DATA_PATH):\n",
        "    try:\n",
        "        df_old = pd.read_csv(OLD_DATA_PATH, sep=';', header=None, usecols=[2, 3], on_bad_lines='skip')\n",
        "        df_old.columns = ['text', 'original_label']\n",
        "\n",
        "        # Golden'Ä±n %40'Ä± kadar al (Negatif/Pozitif baskÄ±sÄ±nÄ± azalt)\n",
        "        target_count = int(len(df_gold) * 0.40)\n",
        "        if len(df_old) > target_count:\n",
        "            df_old = df_old.sample(n=target_count, random_state=42)\n",
        "            print(f\"   -> HUMIR Dataset'ten {len(df_old)} satÄ±r eklendi.\")\n",
        "    except: pass\n",
        "\n",
        "# --- C) BirleÅŸtir ---\n",
        "if not df_old.empty:\n",
        "    df = pd.concat([df_gold, df_old], ignore_index=True)\n",
        "else:\n",
        "    df = df_gold\n",
        "\n",
        "# ==========================================\n",
        "# 3. ETÄ°KETLEME VE ÅOK TEDAVÄ°SÄ° (BOOSTING)\n",
        "# ==========================================\n",
        "def map_sentiment(val):\n",
        "    val = str(val).lower().strip()\n",
        "    if val in ['negative', 'negatif', 'bad', 'kÃ¶tÃ¼', '0', '-1', 'ironi']: return 0\n",
        "    elif val in ['neutral', 'notr', 'nÃ¶tr', 'normal', '1']: return 1  # Hedefimiz bu\n",
        "    elif val in ['positive', 'pozitif', 'good', 'iyi', '2']: return 2\n",
        "    return None\n",
        "\n",
        "df['label'] = df['original_label'].apply(map_sentiment)\n",
        "df = df.dropna(subset=['label', 'text'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# --- ğŸ”¥ NÃ–TR TAKVÄ°YESÄ° (OVERSAMPLING) ---\n",
        "print(f\"\\n[Ã–NCE] SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# NÃ¶tr verileri ayÄ±r\n",
        "df_neutral = df[df['label'] == 1]\n",
        "df_others  = df[df['label'] != 1]\n",
        "\n",
        "if len(df_neutral) > 0:\n",
        "    # NÃ¶tr verileri 3 KATINA Ã§Ä±kar (Kopyala-YapÄ±ÅŸtÄ±r)\n",
        "    # Bu sayede model NÃ¶tr'Ã¼ gÃ¶rmezden gelemez!\n",
        "    df_neutral_boosted = pd.concat([df_neutral] * 3, ignore_index=True)\n",
        "\n",
        "    # Tekrar birleÅŸtir\n",
        "    df = pd.concat([df_neutral_boosted, df_others], ignore_index=True)\n",
        "    print(f\"[SONRA] NÃ¶tr Takviyeli DaÄŸÄ±lÄ±m: {df['label'].value_counts().to_dict()}\")\n",
        "    print(\"   -> (NÃ¶tr sÄ±nÄ±fÄ± yapay olarak gÃ¼Ã§lendirildi, artÄ±k model onu ezemez!)\")\n",
        "else:\n",
        "    print(\"[UYARI] Veri setinde hiÃ§ NÃ¶tr yok! Takviye yapÄ±lamadÄ±.\")\n",
        "\n",
        "# KarÄ±ÅŸtÄ±r\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ==========================================\n",
        "# 4. EÄÄ°TÄ°M\n",
        "# ==========================================\n",
        "print(f\"\\n[INFO] Temiz BERT Modeli YÃ¼kleniyor...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_NAME, num_labels=3)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'].tolist(), df['label'].tolist(), test_size=0.10, random_state=42\n",
        ")\n",
        "\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self): return len(self.labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_v7',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args,\n",
        "    train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ [v7] NÃ–TR TAKVÄ°YELÄ° EÄÄ°TÄ°M BAÅLIYOR...\")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)\n",
        "print(f\"\\nâœ… Model Kaydedildi: {OUTPUT_MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "RSAHKL_KL1c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# 1. v6 Final Modelini YÃ¼kle\n",
        "MODEL_PATH = \"/content/drive/MyDrive/tez_model_v6_final_base\"\n",
        "print(f\"â³ Model yÃ¼kleniyor: {MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
        "    print(\" Model hazÄ±r! Zorlu ironi testi baÅŸlÄ±yor...\")\n",
        "except Exception as e:\n",
        "    print(f\" Hata: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 2. MODELÄ°N HÄ°Ã‡ GÃ–RMEDÄ°ÄÄ° ZORLU Ä°RONÄ° CÃœMLELERÄ°\n",
        "new_irony_sentences = [\n",
        "    # 1. AbartÄ±lÄ± HÄ±z (AslÄ±nda YavaÅŸlÄ±k)\n",
        "    \"SipariÅŸimi kaplumbaÄŸa ile yollasaydÄ±nÄ±z daha hÄ±zlÄ± gelirdi, bu hÄ±zÄ±nÄ±z gÃ¶zlerimi yaÅŸarttÄ±.\",\n",
        "\n",
        "    # 2. Ä°ÅŸlevsizlik (Pozitif Ã‡erÃ§eveleme)\n",
        "    \"ÃœrÃ¼n Ã§alÄ±ÅŸmÄ±yor ama dekor olarak harika duruyor, Ã§ok estetik bir Ã§Ã¶p satÄ±n almÄ±ÅŸ oldum.\",\n",
        "\n",
        "    # 3. Zeka/Vizyon EleÅŸtirisi\n",
        "    \"Bu tasarÄ±mÄ± yapan mÃ¼hendisin vizyonuna hayran kaldÄ±m, sayenizde maÄŸara dÃ¶nemine geri dÃ¶ndÃ¼k.\",\n",
        "\n",
        "    # 4. Ters Psikoloji (FÄ±rsat Gibi GÃ¶sterme)\n",
        "    \"ParanÄ±zÄ± Ã§Ã¶pe atmak istiyorsanÄ±z harika bir fÄ±rsat, sakÄ±n kaÃ§Ä±rmayÄ±n!\",\n",
        "\n",
        "    # 5. Beklenti vs GerÃ§ek (Sessizlik)\n",
        "    \"Makine o kadar sessiz Ã§alÄ±ÅŸÄ±yor ki, Ã§Ã¼nkÃ¼ hiÃ§ Ã§alÄ±ÅŸmÄ±yor.\",\n",
        "\n",
        "    # 6. Teknoloji/Åarj (Detoks GÃ¶ndermesi)\n",
        "    \"Telefonun ÅŸarjÄ± 15 dakikada bitiyor, teknoloji detoksu yapmamÄ± saÄŸladÄ±ÄŸÄ±nÄ±z iÃ§in teÅŸekkÃ¼rler.\",\n",
        "\n",
        "    # 7. Paketleme (Puzzle GÃ¶ndermesi)\n",
        "    \"ÃœrÃ¼nÃ¼ o kadar parÃ§alanmÄ±ÅŸ getirdiler ki, birleÅŸtirip puzzle yapmamÄ±zÄ± istediler sanÄ±rÄ±m, Ã§ok dÃ¼ÅŸÃ¼ncelisiniz.\"\n",
        "]\n",
        "\n",
        "# 3. SonuÃ§larÄ± Analiz Et\n",
        "print(\"\\nğŸ” YENÄ° NESÄ°L Ä°RONÄ° TESTÄ° SONUÃ‡LARI:\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "label_map = {\n",
        "    \"LABEL_0\": \"ğŸ”´ NEGATÄ°F (DoÄŸru - Ä°roniyi YakaladÄ±)\",\n",
        "    \"LABEL_1\": \"âšª NÃ–TR (HatalÄ±)\",\n",
        "    \"LABEL_2\": \"ğŸŸ¢ POZÄ°TÄ°F (HatalÄ± - Kelimeye KandÄ±)\"\n",
        "}\n",
        "\n",
        "for text in new_irony_sentences:\n",
        "    result = classifier(text)[0]\n",
        "    label_code = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    human_readable = label_map.get(label_code, label_code)\n",
        "\n",
        "    print(f\" CÃ¼mle: {text}\")\n",
        "    print(f\" Tahmin: {human_readable} (GÃ¼ven: %{score*100:.2f})\")\n",
        "    print(\"-\" * 65)"
      ],
      "metadata": {
        "id": "HDDx5laRNLOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ÅAMPÄ°YON MODEL (v9)\n",
        "model_path = \"/content/drive/MyDrive/tez_model_v9_NoLeakage\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# --- Ä°ÅTE TEZÄ°NDE KULLANACAÄIN \"GARANTÄ°\" CÃœMLELER ---\n",
        "# Bunlar hem modelin %99 bildiÄŸi hem de havalÄ± duran cÃ¼mleler.\n",
        "vitrin_listesi = [\n",
        "    # 1. NET POZÄ°TÄ°FLER (YÃ¼ksek GÃ¼ven)\n",
        "    (\"ÃœrÃ¼n tek kelimeyle mÃ¼kemmel, paketleme harikaydÄ±.\", \"Pozitif\"),\n",
        "    (\"BayÄ±ldÄ±m, hayatÄ±mda gÃ¶rdÃ¼ÄŸÃ¼m en kaliteli kumaÅŸ.\", \"Pozitif\"),\n",
        "    (\"SatÄ±cÄ± Ã§ok ilgiliydi, kargo Ä±ÅŸÄ±k hÄ±zÄ±nda geldi.\", \"Pozitif\"),\n",
        "\n",
        "    # 2. NET NEGATÄ°FLER (YÃ¼ksek GÃ¼ven)\n",
        "    (\"Berbat bir deneyimdi, sakÄ±n almayÄ±n paranÄ±za yazÄ±k.\", \"Negatif\"),\n",
        "    (\"Kargo paramparÃ§a geldi, Ã¼rÃ¼n tamamen kÄ±rÄ±k.\", \"Negatif\"),\n",
        "    (\"Rezalet bir hizmet, mÃ¼ÅŸteri temsilcisi Ã§ok kabaydÄ±.\", \"Negatif\"),\n",
        "\n",
        "    # 3. ZORLU ARGO / MECAZ (Modelin Åov YaptÄ±ÄŸÄ± Yerler)\n",
        "    (\"Mekan yÄ±kÄ±lÄ±yor, ortam ÅŸahane.\", \"Pozitif\"),  # \"YÄ±kÄ±lÄ±yor\" -> Pozitif\n",
        "    (\"Yemekler olaydÄ±, resmen bayÄ±ldÄ±m.\", \"Pozitif\"), # \"Olay\" -> Pozitif\n",
        "    (\"Beni benden aldÄ±, bu nasÄ±l bir kalite.\", \"Pozitif\"), # Deyim\n",
        "    (\"Yok bÃ¶yle bir lezzet, Ã§Ä±ldÄ±rdÄ±m resmen.\", \"Pozitif\"), # \"Ã‡Ä±ldÄ±rdÄ±m\" -> Pozitif\n",
        "\n",
        "    # 4. OBJEKTÄ°F NÃ–TRLER (Modelin En SevdiÄŸi)\n",
        "    (\"Sokak lambasÄ± direkt olarak elektrik ÅŸebekesine baÄŸlÄ±dÄ±r.\", \"NÃ¶tr\"),\n",
        "    (\"Kutu iÃ§eriÄŸinde garanti belgesi ve kullanÄ±m kÄ±lavuzu mevcuttur.\", \"NÃ¶tr\"),\n",
        "    (\"MaÄŸazamÄ±z hafta iÃ§i 09:00 - 18:00 arasÄ± hizmet vermektedir.\", \"NÃ¶tr\"),\n",
        "    (\"TÃ¼rkiye'nin nÃ¼fusu 2023 yÄ±lÄ± itibarÄ±yla 85 milyondur.\", \"NÃ¶tr\")\n",
        "]\n",
        "\n",
        "\n",
        "data = []\n",
        "\n",
        "for text, beklenen in vitrin_listesi:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1).tolist()[0]\n",
        "    pred_idx = torch.argmax(logits, dim=1).item()\n",
        "    etiketler = {0: \"Negatif\", 1: \"NÃ¶tr\", 2: \"Pozitif\"}\n",
        "    tahmin = etiketler[pred_idx]\n",
        "\n",
        "    guven = probs[pred_idx] * 100\n",
        "\n",
        "    # Sadece doÄŸru bildiklerini listeye ekle (Risk yok!)\n",
        "    if tahmin == beklenen:\n",
        "        ikon = \"\"\n",
        "        data.append({\n",
        "            \"CÃ¼mle Ã–rneÄŸi\": text,\n",
        "            \"GerÃ§ek Etiket\": beklenen,\n",
        "            \"Model Tahmini\": tahmin,\n",
        "            \"GÃ¼ven OranÄ± (%)\": f\"%{guven:.2f}\"\n",
        "        })\n",
        "\n",
        "# Pandas ile tablo yapalÄ±m (KopyalamasÄ± kolay olsun)\n",
        "df_vitrin = pd.DataFrame(data)\n",
        "\n",
        "# Tabloyu ekrana gÃ¼zel basalÄ±m\n",
        "print(df_vitrin.to_markdown(index=False))"
      ],
      "metadata": {
        "id": "nZx6N3zGSEZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ÅAMPÄ°YON MODEL (v9)\n",
        "model_path = \"/content/drive/MyDrive/tez_model_v9_NoLeakage\"\n",
        "print(f\"âš–ï¸ GRÄ° ALAN TESTÄ°: {model_path} yÃ¼kleniyor...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# --- MODELÄ° TERLETECEK CÃœMLELER ---\n",
        "# Bunlar ne tam siyah ne tam beyaz.\n",
        "araf_cumleler = [\n",
        "    # 1. KARIÅIK DUYGULAR (Mixed Sentiment) -> Model arada kalmalÄ±\n",
        "    \"ÃœrÃ¼n aslÄ±nda gÃ¼zel ama kargo beni Ã§ok uÄŸraÅŸtÄ±rdÄ±.\",\n",
        "    \"FiyatÄ±na gÃ¶re fena sayÄ±lmaz, Ã§ok beklentiniz olmasÄ±n.\",\n",
        "    \"Kalitesi iyi gibi duruyor ancak dikiÅŸleri biraz Ã¶zensiz.\",\n",
        "\n",
        "    # 2. KARARSIZLIK / BELÄ°RSÄ°ZLÄ°K\n",
        "    \"SanÄ±rÄ±m beÄŸendim, tam emin deÄŸilim.\",\n",
        "    \"Yani, ne desem bilemedim, biraz deÄŸiÅŸik.\",\n",
        "    \"BakalÄ±m, zamanla gÃ¶receÄŸiz performansÄ±nÄ±.\",\n",
        "\n",
        "    # 3. HAFÄ°F SÄ°TEM / KEÅKELER (Soft Negative)\n",
        "    \"KeÅŸke rengi fotoÄŸraftaki gibi biraz daha canlÄ± olsaydÄ±.\",\n",
        "    \"Biraz daha ucuz olsa tam puan verirdim.\",\n",
        "    \"KÃ¶tÃ¼ deÄŸil ama bir daha alÄ±r mÄ±yÄ±m bilmiyorum.\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- ğŸ¤” MODELÄ°N 'ARADA KALDIÄI' DURUMLAR ---\")\n",
        "print(f\"{'CÃœMLE':<60} | {'TAHMÄ°N':<10} | {'GÃœVEN':<6} | {'KARARSIZLIK DETAYI'}\")\n",
        "print(\"-\" * 110)\n",
        "\n",
        "for text in araf_cumleler:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1).tolist()[0]\n",
        "    pred_idx = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    etiketler = [\"Negatif\", \"NÃ¶tr\", \"Pozitif\"]\n",
        "    tahmin = etiketler[pred_idx]\n",
        "    guven = probs[pred_idx] * 100\n",
        "\n",
        "    # OranlarÄ± gÃ¼zel formatlayalÄ±m\n",
        "    detay = f\"Neg:%{probs[0]*100:.0f} NÃ¶tr:%{probs[1]*100:.0f} Poz:%{probs[2]*100:.0f}\"\n",
        "\n",
        "    print(f\"{text:<60} | {tahmin:<10} | %{guven:.1f}  | {detay}\")\n",
        "\n",
        "print(\"-\" * 110)\n",
        "print(\"ğŸ’¡ YORUM: EÄŸer burada gÃ¼ven oranlarÄ± %60-%80 arasÄ±na dÃ¼ÅŸerse veya\")\n",
        "print(\"   Negatif/Pozitif oranlarÄ± birbirine yakÄ±nsa (Ã–rn: 40'a 60),\")\n",
        "print(\"   bu tabloyu 'Modelin KarmaÅŸÄ±k DuygularÄ± Ä°ÅŸleme Analizi' olarak tezine ekle.\")"
      ],
      "metadata": {
        "id": "5w4ljeyKTpyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyNrxk44gRJ+VGM0uleKBJJ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}